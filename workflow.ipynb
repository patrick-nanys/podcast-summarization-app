{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note before running\n",
    "\n",
    "API KEY will not be pushed to the repo, you should make one for yourself at https://beta.elevenlabs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Egyetem\\ELTE\\3.felev\\AdvancedSoftwareTechnology\\podcast-summarization-app\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kissa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import openai\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytube import YouTube\n",
    "from elevenlabslib import ElevenLabsUser\n",
    "\n",
    "from experiments.asr.asr import whisper_asr\n",
    "from experiments.tts.tts import elevenlabs_tts\n",
    "from experiments.summarization.openai.summarize import summarize_text, num_tokens_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_voices(API_KEY):\n",
    "    user = ElevenLabsUser(API_KEY)\n",
    "    for voice in user.get_available_voices():\n",
    "        print(voice.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_url, destination):\n",
    "    yt = YouTube(video_url, use_oauth=True, allow_oauth_cache=True)\n",
    "    print(f'Downloading video: {yt.title} from author {yt.author}')\n",
    "    video = yt.streams.filter(only_audio=True).first()\n",
    "    out_file = video.download(output_path=destination)\n",
    "    base, ext = os.path.splitext(out_file)\n",
    "    new_target_file_name = base + '.wav'\n",
    "    subprocess.run(['ffmpeg', '-i', out_file, '-ar', '16000', '-ac', '1', '-b:a', '96K', '-acodec', 'pcm_s16le', new_target_file_name])\n",
    "\n",
    "    return new_target_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_and_timestamps(target_dir, target_file_name, output_directory):\n",
    "    '''\n",
    "    Reading back the text files. This should be done later asynchronously/with multithreading\n",
    "    Also saves the transcription df to @param output_directory\n",
    "    '''\n",
    "    target_file_name_base, ext = os.path.splitext(target_file_name)\n",
    "    dfs = []\n",
    "\n",
    "    #This trick is needed to sort the filenames by index instead of alphabetically\n",
    "    correct_files = []\n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if target_file_name_base in file_name and file_name.endswith('csv'):\n",
    "            correct_files.append(file_name)\n",
    "    \n",
    "    base_file_name = correct_files[0][:correct_files[0].rfind('_')]\n",
    "    for file_idx in range(len(correct_files)):\n",
    "        file_name = base_file_name+f'_{file_idx}'+'.csv'\n",
    "        print(file_name)\n",
    "        dfs.append(pd.read_csv(os.path.join(target_dir, file_name), delimiter=';', names=['start', 'end', 'text'], encoding='ISO-8859-1', quoting=csv.QUOTE_NONE,))\n",
    "\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    final_lines = ' '.join(df['text'])\n",
    "\n",
    "    df['text_token_counts'] =  df['text'].map(num_tokens_from_text)\n",
    "    df['token_sum'] = np.cumsum(df['text_token_counts'])\n",
    "\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    df.to_csv(os.path.join(output_directory, target_file_name_base + '_transcription.csv'), index=False, sep=';')\n",
    "\n",
    "    token_sums = [0] + list(df['token_sum'])\n",
    "    timestamp_values = list(df['end'])\n",
    "    timestamp_values.insert(0, df['start'].iloc[0])\n",
    "    timestamps_dict = dict(zip(token_sums, timestamp_values))\n",
    "\n",
    "    return final_lines, timestamps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters\n",
    "\n",
    "video_url = 'https://www.youtube.com/watch?v=LqjVMy2qhRY'\n",
    "video_download_folder = 'downloaded_videos_test'\n",
    "output_directory = 'autogpt_karoly'\n",
    "\n",
    "ELEVENLABS_API_KEY = ''\n",
    "AWS_ACCESS_KEY = ''\n",
    "AWS_SECRET_KEY = ''\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video: AutoGPT: This Is ChatGPT Supercharged! from author Two Minute Papers\n"
     ]
    }
   ],
   "source": [
    "#Download video\n",
    "\n",
    "target_file_name = download_video(video_url, video_download_folder)\n",
    "target_file_name = os.path.basename(target_file_name)\n",
    "target_file_name_base, ext = os.path.splitext(target_file_name)\n",
    "podcast_sound_path = os.path.join(video_download_folder, target_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.20606699999917\n",
      "77.80173500000092\n",
      "11.16660380000394\n"
     ]
    }
   ],
   "source": [
    "#Speech to text\n",
    "\n",
    "whisper_asr(podcast_sound_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGPT This Is ChatGPT Supercharged!.wav_0.csv\n",
      "AutoGPT This Is ChatGPT Supercharged!.wav_1.csv\n",
      "AutoGPT This Is ChatGPT Supercharged!.wav_2.csv\n"
     ]
    }
   ],
   "source": [
    "#Loading transcriptions, saving it\n",
    "\n",
    "text, timestamps_dict = load_files_and_timestamps(video_download_folder, target_file_name, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 1308 tokens long.\n",
      "Final chunks token counts:  [1308]\n",
      "Split the text into 1 chunks.\n",
      "chunk_start_timestamps ['00:00:00.000']\n",
      "Cost of summary was:  0.002996\n"
     ]
    }
   ],
   "source": [
    "#Text summary, saving the generated files to json\n",
    "\n",
    "input_text, chunks, chunk_start_timestamps = summarize_text(text, timestamps_dict)\n",
    "\n",
    "with open(os.path.join(output_directory, target_file_name_base + '_summarized_text.json'), 'w') as f:\n",
    "    json.dump(input_text, f, indent=2)\n",
    "with open(os.path.join(output_directory, target_file_name_base + '_chunks.json'), 'w') as f:\n",
    "    json.dump(chunks, f, indent=2)\n",
    "with open(os.path.join(output_directory, target_file_name_base + '_chunk_start_timestamps.json'), 'w') as f:\n",
    "    json.dump(chunk_start_timestamps, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary length: 652\n",
      "Remaining ElevenLabs characters: 6554\n"
     ]
    }
   ],
   "source": [
    "#Check if ElevenLabs can be used\n",
    "\n",
    "summary_length = sum([len(chunk) for chunk in chunks])\n",
    "print('Summary length:', summary_length)\n",
    "\n",
    "user = ElevenLabsUser(ELEVENLABS_API_KEY)\n",
    "remaining_characters = user.get_character_limit() - user.get_current_character_count()\n",
    "print('Remaining ElevenLabs characters:', remaining_characters)\n",
    "\n",
    "if summary_length > remaining_characters:\n",
    "    raise ValueError('Not enough characters for TTS. Provide an ElevenLabs API token with enough remaining characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TTS with elevenlabs\n",
    "\n",
    "elevenlabs_tts(chunks, ELEVENLABS_API_KEY, 'Adam', os.path.join(output_directory, target_file_name_base + '_read_summary.mp3'))#Male voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to s3 bucket\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = 'breviocast-prod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "podcasts/uranium_mining_outputs/How Uranium Mining Works  STUFF YOU SHOULD KNOW_chunk_start_timestamps.json\n",
      "podcasts/uranium_mining_outputs/How Uranium Mining Works  STUFF YOU SHOULD KNOW_chunks.json\n",
      "podcasts/uranium_mining_outputs/How Uranium Mining Works  STUFF YOU SHOULD KNOW_read_summary.mp3\n",
      "podcasts/uranium_mining_outputs/How Uranium Mining Works  STUFF YOU SHOULD KNOW_summarized_text.json\n",
      "podcasts/uranium_mining_outputs/How Uranium Mining Works  STUFF YOU SHOULD KNOW_transcription.csv\n",
      "test.txt\n",
      "uranium_mining_outputs\\How Uranium Mining Works  STUFF YOU SHOULD KNOW_read_summary.mp3\n",
      "uranium_mining_outputs\\How Uranium Mining Works  STUFF YOU SHOULD KNOW_transcription.csv\n",
      "uranium_mining_outputs\\How Uranium Mining Works  STUFF YOU SHOULD KNOWchunk_start_timestamps.json\n",
      "uranium_mining_outputs\\How Uranium Mining Works  STUFF YOU SHOULD KNOWchunks.json\n",
      "uranium_mining_outputs\\How Uranium Mining Works  STUFF YOU SHOULD KNOWsummarized_text.json\n"
     ]
    }
   ],
   "source": [
    "def list_files_in_bucket(bucket_name, s3):\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    for my_bucket_object in my_bucket.objects.all():\n",
    "        print(my_bucket_object.key)\n",
    "\n",
    "def upload_directory_to_bucket(path, bucket_name, s3):\n",
    "    for file_name in os.listdir(path):\n",
    "        s3.meta.client.upload_file(os.path.join(path,file_name), bucket_name, f'podcasts/{path}/{file_name}')\n",
    "\n",
    "upload_directory_to_bucket(output_directory, bucket_name, s3)\n",
    "list_files_in_bucket(bucket_name, s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
