{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note before running\n",
    "\n",
    "API KEY will not be pushed to the repo, you should make one for yourself at https://beta.elevenlabs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import openai\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytube import YouTube\n",
    "from elevenlabslib import ElevenLabsUser\n",
    "\n",
    "from experiments.asr.asr import whisper_asr\n",
    "from experiments.tts.tts import elevenlabs_tts\n",
    "from experiments.summarization.openai.summarize import summarize_text, num_tokens_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_voices(API_KEY):\n",
    "    user = ElevenLabsUser(API_KEY)\n",
    "    for voice in user.get_available_voices():\n",
    "        print(voice.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_url, destination):\n",
    "    yt = YouTube(video_url, use_oauth=True, allow_oauth_cache=True)\n",
    "    print(f'Downloading video: {yt.title} from author {yt.author}')\n",
    "    video = yt.streams.filter(only_audio=True).first()\n",
    "    out_file = video.download(output_path=destination)\n",
    "    base, ext = os.path.splitext(out_file)\n",
    "    new_target_file_name = base + '.wav'\n",
    "    subprocess.run(['ffmpeg', '-i', out_file, '-ar', '16000', '-ac', '1', '-b:a', '96K', '-acodec', 'pcm_s16le', new_target_file_name])\n",
    "\n",
    "    return new_target_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_and_timestamps(target_dir, target_file_name, output_directory):\n",
    "    '''\n",
    "    Reading back the text files. This should be done later asynchronously/with multithreading\n",
    "    Also saves the transcription df to @param output_directory\n",
    "    '''\n",
    "    target_file_name_base, ext = os.path.splitext(target_file_name)\n",
    "    dfs = []\n",
    "\n",
    "    #This trick is needed to sort the filenames by index instead of alphabetically\n",
    "    correct_files = []\n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if target_file_name_base in file_name and file_name.endswith('csv'):\n",
    "            correct_files.append(file_name)\n",
    "    \n",
    "    base_file_name = correct_files[0][:correct_files[0].rfind('_')]\n",
    "    for file_idx in range(len(correct_files)):\n",
    "        file_name = base_file_name+f'_{file_idx}'+'.csv'\n",
    "        print(file_name)\n",
    "        dfs.append(pd.read_csv(os.path.join(target_dir, file_name), delimiter=';', names=['start', 'end', 'text'], encoding='ISO-8859-1', quoting=csv.QUOTE_NONE,))\n",
    "\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    final_lines = ' '.join(df['text'])\n",
    "\n",
    "    df['text_token_counts'] =  df['text'].map(num_tokens_from_text)\n",
    "    df['token_sum'] = np.cumsum(df['text_token_counts'])\n",
    "\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    df.to_csv(os.path.join(output_directory, 'transcription.csv'), index=False, sep=';')\n",
    "\n",
    "    token_sums = [0] + list(df['token_sum'])\n",
    "    timestamp_values = list(df['end'])\n",
    "    timestamp_values.insert(0, df['start'].iloc[0])\n",
    "    timestamps_dict = dict(zip(token_sums, timestamp_values))\n",
    "\n",
    "    return final_lines, timestamps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters\n",
    "\n",
    "video_url = \"\"\n",
    "video_download_folder = \"\"\n",
    "output_directory = \"\"\n",
    "\n",
    "ELEVENLABS_API_KEY = ''\n",
    "AWS_ACCESS_KEY = ''\n",
    "AWS_SECRET_KEY = ''\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download video\n",
    "\n",
    "target_file_name = download_video(video_url, video_download_folder)\n",
    "target_file_name = os.path.basename(target_file_name)\n",
    "target_file_name_base, ext = os.path.splitext(target_file_name)\n",
    "podcast_sound_path = os.path.join(video_download_folder, target_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speech to text\n",
    "\n",
    "whisper_asr(podcast_sound_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading transcriptions, saving it\n",
    "\n",
    "text, timestamps_dict = load_files_and_timestamps(video_download_folder, target_file_name, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text summary, saving the generated files to json\n",
    "\n",
    "input_text, chunks, chunk_start_timestamps = summarize_text(text, timestamps_dict)\n",
    "\n",
    "with open(os.path.join(output_directory, 'summarized_text.json'), 'w') as f:\n",
    "    json.dump(input_text, f, indent=2)\n",
    "with open(os.path.join(output_directory, 'chunks.json'), 'w') as f:\n",
    "    json.dump(chunks, f, indent=2)\n",
    "with open(os.path.join(output_directory, 'chunk_start_timestamps.json'), 'w') as f:\n",
    "    json.dump(chunk_start_timestamps, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if ElevenLabs can be used\n",
    "\n",
    "summary_length = sum([len(chunk) for chunk in chunks])\n",
    "print('Summary length:', summary_length)\n",
    "\n",
    "user = ElevenLabsUser(ELEVENLABS_API_KEY)\n",
    "remaining_characters = user.get_character_limit() - user.get_current_character_count()\n",
    "print('Remaining ElevenLabs characters:', remaining_characters)\n",
    "\n",
    "if summary_length > remaining_characters:\n",
    "    raise ValueError('Not enough characters for TTS. Provide an ElevenLabs API token with enough remaining characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TTS with elevenlabs\n",
    "\n",
    "elevenlabs_tts(chunks, ELEVENLABS_API_KEY, 'Adam', os.path.join(output_directory, 'read_summary.mp3'))#Male voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to s3 bucket\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = 'breviocast-prod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_bucket(bucket_name, s3):\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    for my_bucket_object in my_bucket.objects.all():\n",
    "        print(my_bucket_object.key)\n",
    "\n",
    "def upload_directory_to_bucket(path, bucket_name, s3):\n",
    "    for file_name in os.listdir(path):\n",
    "        s3.meta.client.upload_file(os.path.join(path,file_name), bucket_name, f'podcasts/{path}/{file_name}')\n",
    "\n",
    "upload_directory_to_bucket(output_directory, bucket_name, s3)\n",
    "list_files_in_bucket(bucket_name, s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
