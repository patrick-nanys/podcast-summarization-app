{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note before running\n",
    "\n",
    "elevenlabslib and pydub are not in the requirements.txt yet, because I'm not sure whether we will use that or the speech5 tts method, since elevenlabs is a paid service.  \n",
    "To run this notebook, follow the instructions in run.md, and also: \n",
    "```\n",
    "pip install elevenlabslib pydub\n",
    "```\n",
    "\n",
    "API KEY will not be pushed to the repo, you should make one for yourself at https://beta.elevenlabs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabslib import ElevenLabsUser\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "from asr import whisper_asr\n",
    "from tts import speecht5_tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy summarization using a simple model from huggingface\n",
    "\n",
    "def summarize(text):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    summary = summarizer(text)\n",
    "    summarized_text = summary[0]['summary_text']\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElevenLabs TTS: This could be costly. Voice cloning is only a paid feature, free TTS: 10000 characters/month\n",
    "\n",
    "def elevenlabs_tts(input_text, API_KEY, voice_name, output_path):\n",
    "    user = ElevenLabsUser(API_KEY)\n",
    "    voice = user.get_voices_by_name(voice_name)[0]\n",
    "    returned_value = voice.generate_audio_bytes(input_text)\n",
    "\n",
    "    recording = AudioSegment.from_file(io.BytesIO(returned_value), format=\"mp3\")\n",
    "    recording.export(output_path, format='mp3') #ElevenLabs always returns mp3 bytes. Maybe it should be converted to wav with ffmpeg?\n",
    "\n",
    "def list_available_voices(API_KEY):\n",
    "    user = ElevenLabsUser(API_KEY)\n",
    "    for voice in user.get_available_voices():\n",
    "        print(voice.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.96806900000001\n",
      "57.461708000000044\n"
     ]
    }
   ],
   "source": [
    "#Speech to text\n",
    "\n",
    "target_file_name = '2minutepaper.wav'\n",
    "whisper_asr(target_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2minutepaper.wav_0.csv\n",
      "2minutepaper.wav_1.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Dear Fellow Scholars, this is 2 Minute Papers with Dr. Károly Zsolnai-Fehér. This is GPT-4, OpenAI's new language model AI that we just talked about and today I would love to show you that it has barely been out and it is already taking the world by storm. Here are 5 incredible ways it already is in use in the real world, in real products, some of which you can probably try right now or at the very least soon. One, in the Language Learning app Duolingo, they now supercharge the process of learning a new language with GPT-4. We can have scripted conversations, for instance, we can order coffee in French. That is very cool, but that's not the interesting part yet. Now look, GPT-4 will look at our conversation and give us tips on how to make our French feel a little more natural. And don't forget, this can be a two-way conversation, so if we don't quite understand what it is trying to teach us, we can also ask for additional examples. Loving it! Two, Stripe, a payment processor company, puts GPT-4's amazing information organizing capabilities to use by letting the users ask them a question in natural text. Now, normally, if we have a question like this, we would have to look through the entire documentation and hope there is something about it. But from today on, not anymore. Instead, we can now ask GPT-4 to look through their entire documentation and summarize the solution for us. This is going to save a lot of time for a lot of people. Three, my favorite, Can Academy is also testing a GPT-4 powered solution. If you are a student and you have a question about, for instance, mathematics, it will behave like a real tutor, and the best part is, it will not just give you the solution, but it can also try to steer you towards that solution and give you hints so you can find it out yourself. This is going to help millions and millions of children study all around the world. And it can also help with perhaps the most dreaded question a computer science student can ask. And of course, that is, here is my piece of code, but it's not working. What's wrong with it? And it can help us with that too. That is absolutely amazing. What a time to be alive! And it can also help teachers create a curriculum too. Four, while we are talking about helping us write and fix computer code, it can also look at our whole codebase and whenever some issue arises, it can not only tell us what is going on, but it will actually write that missing subtraction function and fix the return variable as well in one go. So cool. And five, it can also be connected to your car and we can ask it about our average speed and the  But that would not be as useful. Can it do something that requires a little more intelligence? Of course it can. Look, it can even create a plot that shows our speed and our battery drain for each of our previous driving sessions. And that is just the first few experiments of trying to make this incredible new AI work. And it already helps us learn new languages, organize our knowledge, it can become an excellent tutor when studying, helps us fixing computer code and also makes our car more useful. How cool is that? And don't forget GPT-4 is multi-model, so we can not only give it text as an input, but images too. We can ask it to explain memes or tell us what would happen in hypothetical scenarios. And just imagine what this will be able to do just two more papers down the line. Maybe a personal assistant who reads news and papers for us, it knows our interests and summarizes them just in the way we like it, checks its sources and points out potential flaws in the reasoning. I would love that. But you know what? Perhaps GPT-4 can already do that, we just need to find a way to ask it properly. That is the incredible world we live in today. What a time to be alive! If you are looking for inexpensive cloud GPUs for AI, Lambda now offers the best prices in the world for GPU cloud compute. No commitments or negotiation required. Just sign up and launch an instance. And hold on to your papers, because with Lambda GPU cloud, you can get on-demand A100 instances for $1.10 per hour versus $4.10 per hour with AWS. That's 73% savings. Did I mention they also offer persistent storage, so join researchers at organizations like Apple, MIT and Caltech in using Lambda cloud instances, workstations or servers. Make sure to go to lambdalabs.com/papers to sign up for one of their amazing GPU instances today. Thanks for watching and for your general support, and I'll see you next time!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading back the text files. This should be done later asynchronously/with multithreading\n",
    "target_file_name_base, ext = os.path.splitext(target_file_name)\n",
    "dfs = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if target_file_name_base in file and file.endswith('csv'):\n",
    "        print(file)\n",
    "        dfs.append(pd.read_csv(file, delimiter=';', names=['start', 'end', 'text'], encoding='ISO-8859-1'))\n",
    "\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "df\n",
    "\n",
    "final_lines = ' '.join(df['text'])\n",
    "final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" GPT-4, OpenAI's new language model AI, has barely been out and it is already taking the world by storm . It already helps us learn new languages, organize our knowledge, it can become an excellent tutor when studying, helps us fixing computer code and also makes our car more useful .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary\n",
    "\n",
    "input_text = summarize(final_lines)\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Egyetem\\ELTE\\3.felev\\AdvancedSoftwareTechnology\\podcast-summarization-app\\venv\\lib\\site-packages\\torch\\functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ..\\aten\\src\\ATen\\native\\SpectralOps.cpp:867.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "Found cached dataset cmu-arctic-xvectors (C:/Users/kissa/.cache/huggingface/datasets/Matthijs___cmu-arctic-xvectors/default/0.0.1/a62fea1f9415e240301ea0042ffad2a3aadf4d1caa7f9a8d9512d631723e781f)\n"
     ]
    }
   ],
   "source": [
    "#TTS with 2 (and a half) methods\n",
    "\n",
    "speecht5_tts(input_text, embedding_type = 'custom', custom_embedding_path = '2minutepaper.wav', output_path = 'speecht5_tts_2minute_paper_voice_clone.wav') #Trying voice cloning here\n",
    "speecht5_tts(input_text, embedding_type = 'female1', output_path = 'speecht5_tts_2minute_paper.wav') #Basic female voice\n",
    "elevenlabs_tts(input_text, API_KEY, 'Bella', 'elevenlabs_tts_2minute_paper.mp3') #Female voice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
